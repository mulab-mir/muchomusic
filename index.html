<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriate as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description"
          content="MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models - MuChoMusic is a benchmark for evaluating music understanding in multimodal audio-language models.">
    <meta property="og:title" content="MuCho Music"/>
    <meta property="og:description"
          content="MuChoMusic is a benchmark for evaluating music understanding in multimodal audio-language models."/>
    <meta property="og:url" content="https://mulab-mir.github.io/muchomusic/"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
    <!--    <meta property="og:image" content="static/image/your_banner_image.png"/>-->
    <!--    <meta property="og:image:width" content="1200"/>-->
    <!--    <meta property="og:image:height" content="630"/>-->


    <meta name="twitter:title" content="MuCho Music">
    <meta name="twitter:description"
          content="MuChoMusic is a benchmark for evaluating music understanding in multimodal audio-language models.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
    <!--    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">-->
    <!--    <meta name="twitter:card" content="summary_large_image">-->
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="deep-learning,audio,language">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">MuChoMusic: <br/>
                        Evaluating Music Understanding in Multimodal Audio-Language Models</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                <a href="https://www.upf.edu/web/mtg/about/team-members/-/asset_publisher/l2XuyhfmWvQ5/content/weck-benno/maximized"
                   target="_blank">Benno Weck</a><sup>*</sup><sup>1</sup>,</span>
                        <span class="author-block">
                  <a href="https://ilariamanco.com" target="_blank">Ilaria Manco</a><sup>*</sup><sup>2,3</sup>,</span>
                        <span class="author-block">
                    <a href="https://www.eecs.qmul.ac.uk/~emmanouilb" target="_blank">Emmanouil Benetos</a><sup>2</sup>,</span>
                        <span class="author-block">
                    <a href="https://elioquinton.github.io" target="_blank">Elio Quinton</a><sup>3</sup>,</span>
                        <span class="author-block">
                    <a href="https://www.eecs.qmul.ac.uk/~gyorgyf/about.html"
                       target="_blank">George Fazekas</a><sup>2</sup>,</span>
                        <span class="author-block">
                        <a href="https://dbogdanov.com" target="_blank">Dmitry Bogdanov</a><sup>1</sup>
                    </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>UPF, <sup>2</sup>QMUL, <sup>3</sup>UMG<br>ISMIR 2024</span>
                        <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                           class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                            <!-- Data link -->
                            <span class="link-block">
                    <a href="https://doi.org/10.5281/zenodo.12709974" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                    </a>
                </span>

                            <!-- Github link -->
                            <span class="link-block">
                    <a href="https://github.com/mulab-mir/muchomusic" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                            <!-- Supplementary PDF link -->
                            <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                    </a>
                </span>


                            <!-- ArXiv abstract Link
                            <span class="link-block">
                              <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                              class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                <i class="ai ai-arxiv"></i>
                              </span>
                              <span>arXiv</span>
                            </a>
                          </span>-->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="columns is-centered">
                <!-- center the image -->
                <img src="./static/images/muchomusic.png" alt="Teaser" class="teaser-image center" width="50%"/>
            </div>
        </div>
    </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Overview</h2>
                <div class="content has-text-justified">
                    <p>
                        <span class="dnerf">MuChoMusic</span> is a benchmark for evaluating music understanding in
                        multimodal audio-language models. It comprises 1,187 multiple-choice questions, all validated by
                        human annotators, associated with 644 music tracks sourced from two publicly available music
                        datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to
                        assess knowledge and reasoning abilities across several dimensions that cover fundamental
                        musical concepts and their relation to cultural and functional contexts. Each question comes
                        with three distractors composed to test different aspects of language and audio understanding.
                        In the knowledge category, questions probe a model's ability to recognise pre-acquired knowledge
                        across various musical aspects. Questions that test reasoning are instead designed to require
                        the synthesis and analytical processing of multiple musical concepts.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Dataset Construction</h2>
                <div class="content has-text-justified">
                    <p>
                        [TODO]
                    </p>
                    <!-- side by side images-->
                    <div class="columns is-centered">
                        <img src="./static/images/generation_example.png" alt="QA generation example"
                             class="teaser-image"
                             width="60%" height="100%" class="center"/>
                    </div>
                    <p>
                        [TODO]
                    </p>
                    <div class="columns is-centered">
                        <img src="./static/images/eval_dims.pdf" alt="Evaluation dimensions" class="teaser-image"
                             width="60%" height="100%" class="center"/>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Results</h2>
                <div class="content has-text-justified">
                    <p>
                        Using <span class="dnerf">MuChoMusic</span>, we evaluate five open-source models, three
                        specialised in the music domain and two general-purpose, and find that Qwen-Audio achieves the
                        highest scores on most dimensions.
                    </p>
                    <!-- side by side images-->
                    <div class="columns is-centered">
                        <img src="./static/images/finegrained_results.png" alt="Finegrained Results"
                             class="teaser-image"
                             width="60%" height="100%" class="center"/>
                    </div>
                    <p>
                        We observe that even the best models can only answer less than 50% of the questions correctly.
                        Surprisingly, out of those considered, models trained on music-specific tasks tend to overall
                        perform worse than those trained on a wider variety of general-audio tasks including speech and
                        everyday sounds.
                    </p>
                    <div class="columns is-centered">
                        <img src="./static/images/results.png" alt="Overview of Results" class="teaser-image"
                             width="60%" height="100%" class="center"/>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Insights</h2>
                <div class="content has-text-justified">
                    <p>
                        In an attempt to understand why models perform poorly, we analyse how results change when using
                        only a single distractor (a) or when passing perturbed audio (b).
                    </p>
                    <!-- side by side images-->
                    <div class="columns is-centered">
                        <img src="./static/images/distractors.png"
                             alt="Experiments with distractors and audio perturbations" class="teaser-image"
                             width="100%" height="100%" class="center"/>
                    </div>

                    <p>
                        From both these experiments, we discover an over-reliance on the language modality pointing to a
                        need for better multimodal integration.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>


<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
    @inproceedings{weck2024muchomusic,
        title={MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models},
        author={Weck, Benno and Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, György and Bogdanov, Dmitry},
        booktitle = {Proceedings of the 25th International Society for Music Information Retrieval Conference (ISMIR)},
        year={2024}
    }
    </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/"
                                                            target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                        <br>
                        <br>
                        It was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                                  target="_blank">Academic Project Page Template</a> which was adopted
                        from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
