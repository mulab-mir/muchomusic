<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MuChoMusic: <br/> Evaluating Music Understanding in Multimodal Audio-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.upf.edu/web/mtg/about/team-members/-/asset_publisher/l2XuyhfmWvQ5/content/weck-benno/maximized" target="_blank">Benno Weck</a><sup>*</sup><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://ilariamanco.com" target="_blank">Ilaria Manco</a><sup>*</sup><sup>2,3</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.eecs.qmul.ac.uk/~emmanouilb" target="_blank">Emmanouil Benetos</a><sup>2</sup>,</span>
                    <span class="author-block">
                    <a href="https://elioquinton.github.io" target="_blank">Elio Quinton</a><sup>3</sup>,</span>
                    <span class="author-block">
                    <a href="https://www.eecs.qmul.ac.uk/~gyorgyf/about.html" target="_blank">George Fazekas</a><sup>2</sup>,</span>
                    <span class="author-block">
                        <a href="https://dbogdanov.com" target="_blank">Dmitry Bogdanov</a><sup>1</sup>
                    </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>UPF, <sup>2</sup>QMUL, <sup>3</sup>UMG<br>ISMIR 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                <!-- Data link -->
                <span class="link-block">
                    <a href="https://doi.org/10.5281/zenodo.12709974" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                    </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/mulab-mir/muchomusic" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                    <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                    </a>
                </span>


                <!-- ArXiv abstract Link
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered">
          <!-- center the image -->
          <img src="./static/images/muchomusic.png" alt="Teaser" class="teaser-image center" width="50%" />
        </div>
      </div>
    </div>
  </section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <span class="dnerf">MuChoMusic</span> is a benchmark for evaluating music understanding in multimodal audio-language models. It comprises 1,187 multiple-choice questions, all validated by human annotators, associated with 644 music tracks sourced from two publicly available music datasets, and covering a wide variety of genres. Questions in the benchmark are crafted to assess knowledge and reasoning abilities across several dimensions that cover fundamental musical concepts and their relation to cultural and functional contexts. Each question comes with three distractors composed to test different aspects of language and audio understanding. In the knowledge category, questions probe a model's ability to recognise pre-acquired knowledge across various musical aspects. Questions that test reasoning are instead designed to require the synthesis and analytical processing of multiple musical concepts.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <p>
            Using <span class="dnerf">MuChoMusic</span>, we evaluate five open-source models, three specialised in the music domain and two general-purpose, and find that Qwen-Audio achieves the highest scores on most dimensions.
            </p>
            <!-- side by side images-->
            <div class="columns is-centered">
                <img src="./static/images/finegrained_results.png" alt="Finegrained Results" class="teaser-image"
                width="60%" height="100%" class="center" />
            </div>
            <p>
            We observe that even the best models can only answer less than 50% of the questions correctly.
            Surpringly, out of those considered, models trained on music-specific tasks tend to overall perform worse than those trained on a wider variety of general-audio tasks including speech and everyday sounds.
            </p>
            <div class="columns is-centered">
              <img src="./static/images/results.png" alt="Overview of Results" class="teaser-image"
                width="60%" height="100%" class="center" />
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="section">
<div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2 class="title is-3">Insights</h2>
        <div class="content has-text-justified">
        <p>
            In an attempt to understand why models perform poorly, we analyse how results change when using only a single distractor (a) or when passing perturbed audio (b). 
        </p>
        <!-- side by side images-->
        <div class="columns is-centered">
            <img src="./static/images/distractors.png" alt="Experiments with distractors and audio perturbations" class="teaser-image"
            width="100%" height="100%" class="center" />
        </div>

        <p>
            From both these experiments, we discover an over-reliance on the language modality pointing to a need for better multimodal integration. 
        </p>
        </div>
    </div>
    </div>
</div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
    @inproceedings{weck2024muchomusic,
        title={MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language Models},
        author={Weck, Benno and Manco, Ilaria and Benetos, Emmanouil and Quinton, Elio and Fazekas, György and Bogdanov, Dmitry},
        booktitle = {Proceedings of the 25th International Society for Music Information Retrieval Conference (ISMIR)},
        year={2024}
    }
    </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
            <br> 
            It was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
